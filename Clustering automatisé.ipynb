{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Si cela ne marche pas, vous devez installer le modèle de spacy avec\n",
    "# la commande suivante dans le powershell anaconda:\n",
    "# python -m spacy download fr_core_news_lg\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "\n",
    "# Code pour générer le corpus tokenisé à partir du corpus TXT\n",
    "files = os.listdir(\"CORPUS TXT\")\n",
    "for filename in files:\n",
    "    list_ = open(\"CORPUS TXT/\"+filename, encoding='utf-8').read().split(' ')\n",
    "    separator = ' '\n",
    "    txt = separator.join(list_)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(txt)\n",
    "    stopWords = set(stopwords.words('french'))\n",
    "    clean_words = []\n",
    "    for token in tokenized:\n",
    "        if token not in stopWords:\n",
    "            clean_words.append(token)\n",
    "    with open(\"CORPUS TOKEN/out_\" + filename, \"a\") as txtfile:\n",
    "        for word in clean_words:\n",
    "            print(word, file=txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de lemmatisation\n",
    "def return_lemma(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [str(X.lemma_) for X in doc if X.pos_ in(('NOUN','VERB','ADJ')) and str(X) != '…' and not X.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour générer le corpus lemmatisé à partir du corpus TXT\n",
    "files = os.listdir(\"CORPUS TXT\")\n",
    "for filename in files:\n",
    "    list_ = open(\"CORPUS TXT/\"+filename, encoding='utf-8').read().split(' ')\n",
    "    separator = ' '\n",
    "    txt = separator.join(list_)\n",
    "    lemma_list = return_lemma(txt)\n",
    "    with open(\"CORPUS LEMMA/L_out_\" + filename, \"a\", encoding=\"utf-8\") as txtfile:\n",
    "        for lemma in lemma_list:\n",
    "            print(lemma, file=txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Cette partie de code est dediée à calculer la matrice de distances entre\n",
    "# chaque fichier txt du corpus lemmatisé.\n",
    "# REMARQUE: Cela peut prendre un peu de temps!\n",
    "# REMARQUE2: On utilise la similarité cosinus comme distance\n",
    "files = os.listdir(\"CORPUS LEMMA\")\n",
    "matrix_finale = []\n",
    "for filename in files:\n",
    "    distances = []\n",
    "    file1 = open(\"CORPUS LEMMA/\"+filename, encoding='utf-8').read().split(' ')\n",
    "    txt1 = ' '.join(file1)\n",
    "    doc1 = nlp(txt1)\n",
    "    for filename2 in files:\n",
    "        file2 = open(\"CORPUS LEMMA/\"+filename2, encoding='utf-8').read().split(' ')\n",
    "        txt2 = ' '.join(file2)\n",
    "        doc2 = nlp(txt2)\n",
    "        distances.append((1-doc1.similarity(doc2)))\n",
    "    matrix_finale.append(distances)\n",
    "matrix_finale\n",
    "\n",
    "import pickle\n",
    "# Code pour enregistrer la matrice de distances comme un fichier pkl\n",
    "# pour ne pas avoir besoin de faire marcher le code d'avant à chaque fois\n",
    "with open('matrix.pkl', 'wb') as file:\n",
    "    pickle.dump(matrix_finale, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour charger la matrice de distances\n",
    "import pickle\n",
    "with open('matrix.pkl', 'rb') as file:\n",
    "    matrix_finale = pickle.load(file)\n",
    "    print(matrix_finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.spatial.distance as ssd\n",
    "from matplotlib import pyplot as plot\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# Liste avec les noms des fichiers lemmatisés\n",
    "files = os.listdir(\"CORPUS LEMMA\")\n",
    "\n",
    "# Conversion de la matrice entière en une matrice condensée\n",
    "distArray = ssd.squareform(matrix_finale)\n",
    "\n",
    "# On pourrait utiliser aussi le paquet sklearn, mais on a utilisé le paquet scipy car l'affichage avec celui-ci est\n",
    "# plus facile\n",
    "Z = linkage(distArray, 'average')\n",
    "figure = plot.figure(figsize=(50, 30))\n",
    "dendro = dendrogram(Z, leaf_rotation=0,labels=files, leaf_font_size =12, orientation = 'right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
